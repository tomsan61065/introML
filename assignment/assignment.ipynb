{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "numpy, 基礎的package，方便科學運算(ex:矩陣運算).\n",
    "\n",
    "h5py, 讀取資料用\n",
    "\n",
    "pil & scipy, 在這邊使用其畫圖功能.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image #python3 要裝 pillow\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Problem set 介紹##\n",
    "\n",
    "**Problem Statement**:\n",
    "    - Training set = m_train images labeled as cat (y=1) or non-cat (y=0)\n",
    "    - Test set = m_test images labeled as cat or non-cat\n",
    "    - 每一張照片之 shape 為 (num_px, num_px, 3), num_px表示的是像素的數量, 3 表示的是RGB(Red, Green, Blue). \n",
    "\n",
    "作業之目的是要建立一個模型並透過像素的資訊來去預測這張照片是不是貓咪的照片。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "這邊的 train_set_x 以及 test_set_x 之所以後面多加了 \"orig\"，是因為我們會在之後對x進行前處理（將其攤平）。\n",
    "\n",
    "traix_set_x_orig[index] 和 test_set_x_orig[index]，都是表示一張完整的照片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 7\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** 請找出下列三個變數之值:\n",
    "    - m_train (number of training examples)\n",
    "    - m_test (number of test examples)\n",
    "    - num_px (= height = width of a training image)\n",
    "\n",
    "**提示:**\n",
    "    - `train_set_x_orig` 是一個np.array且shape為(m_train, num_px, num_px, 3)，你可以透過`train_set_x_orig.shape[0]`來去知道`m_train`之維度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3)\n",
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "print(train_set_x_orig.shape) # 209 筆 64*64 RGB 圖片\n",
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig[0].shape[0]\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output for m_train, m_test and num_px**: \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td>**m_train**</td>\n",
    "    <td> 209 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**m_test**</td>\n",
    "    <td> 50 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**num_px**</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來就是將 train_set_x_orig 攤平來方便使用，目前其shape為(example數量, num_px, num_px, 3)，我們的目標為將其shape轉成\n",
    "(feature數量(num_px $*$ num_px $*$ 3), example數量).\n",
    "\n",
    "**Exercise:** \n",
    "\n",
    "對training and test data sets做reshape，使其shape從  (num_px, num_px, 3) -> (num\\_px $*$ num\\_px $*$ 3, 1).\n",
    "\n",
    "**提示:**\n",
    "\n",
    "可以透過下列這個指令使得 shape(a, b, c, d) -> shape (b$*$c$*$d, a)\n",
    "\n",
    "```python\n",
    "X.reshape(X.shape[0], -1).T #.T表示取轉置，若沒取轉置，shape為(a, b$*$c$*$d)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n",
      "sanity check after reshaping: [17 31 56 22 33]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "train_set_x_flatten = train_set_x_orig.reshape(209, -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(50, -1).T\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td>**train_set_x_flatten shape**</td>\n",
    "    <td> (12288, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**train_set_y shape**</td>\n",
    "    <td>(1, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**test_set_x_flatten shape**</td>\n",
    "    <td>(12288, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**test_set_y shape**</td>\n",
    "    <td>(1, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>**sanity check after reshaping**</td>\n",
    "  <td>[17 31 56 22 33]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常在前處理的時候我們要對資料集做標準化，標準化有很多種方式，但對於圖像資料集來說，一個簡單的標準化方式就是除以像素的最大值255。\n",
    "(x中的每一個特徵表示一個像素，而這些像素的範圍是 0~255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "常見對資料集的前處理作法為:\n",
    "- 清楚資料維度(shape)\n",
    "- 對資料維度進行處理(reshape)\n",
    "- 標準化資料(/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Learning algorithm 之架構##\n",
    "\n",
    "\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**接下來之步驟:**\n",
    "    - 初始化模型參數\n",
    "    - 測試怎麼樣的參數可以使得模型之cost為最小 \n",
    "    - 透過學習到的模型來進行預測\n",
    "    - 對預測結果進行分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 分別建立演算法之元件 ## \n",
    "\n",
    "通常建立神經網路的步驟為:\n",
    "1. 建立模型架構 \n",
    "2. 初始化模型參數\n",
    "3. 迴圈:\n",
    "    - 計算目前之 loss (forward propagation)\n",
    "    - 計算目前之 gradient (backward propagation)\n",
    "    - 更新參數 (gradient descent)\n",
    "\n",
    "通常我們都分別建構 1-3 然後再將其組裝在 `model()` 內.\n",
    "\n",
    "### 4.1 - 方便運算之函式\n",
    "\n",
    "**Exercise**: \n",
    "實作 `sigmoid()`. 為了方便我們以後計算 $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$. 請使用 np.exp() 才能對矩陣的每一個元素進行exp()運算."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s  = 1/(1+np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>**sigmoid([0, 2])**</td>\n",
    "    <td> [ 0.5         0.88079708]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - 初始化參數\n",
    "\n",
    "**Exercise:** \n",
    "\n",
    "對參數做初始化，請將w初始化為0向量(w之維度取決於特徵之數量)，b初始化為零。\n",
    "\n",
    "請透過np.zeros()的方式來初始化w。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w, b = np.zeros((dim, 1)), 0.0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]]\n",
      "b = 0.0\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table style=\"width:15%\">\n",
    "    <tr>\n",
    "        <td>  ** w **  </td>\n",
    "        <td> [[ 0.]\n",
    " [ 0.]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** b **  </td>\n",
    "        <td> 0 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "對這次的input來說，w之維度應為 (num_px $\\times$ num_px $\\times$ 3, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Forward and Backward propagation\n",
    "\n",
    "完成參數初始化後，接下來我們要透過 \"forward\" 和 \"backward\" propagation的方式去更新並學習參數。\n",
    "\n",
    "**Exercise:** \n",
    "\n",
    "實作`propagate()`來計算 cost 以及 gradient。\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Forward Propagation:\n",
    "- 得到 X(training set)\n",
    "- 計算 $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$ (A表示使用目前的參數來對training set預測的結果，a0表示是第一個training example之預測結果。)\n",
    "- 計算 cost : $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "下面兩個公式可以幫助你計算gradient(w是一個向量其內含對所有個別特徵之參數): \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # w.T -> 變成 row: [1 2], X 是 2 by n 個\n",
    "    A = sigmoid(np.dot(w.T,X)+b)    # compute activation\n",
    "    # Y = [[1, 0]],\n",
    "    # A = [[0.99, 1.0]]\n",
    "    #print(A)\n",
    "    cost = (-1/m)*np.sum( np.dot(Y, np.log(A).T) + np.dot((np.ones(m) - Y), np.log(np.ones(m) - A).T)  )# compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = (1/m)*np.dot(X, (A - Y).T)\n",
    "    db = (1/m)*np.sum(A - Y)\n",
    "    ### END CODE HERE ###\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99993216]\n",
      " [1.99980262]]\n",
      "db = 0.49993523062470574\n",
      "cost = 6.000064773192205\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "    <tr>\n",
    "        <td>  ** dw **  </td>\n",
    "        <td> [[ 0.99993216]\n",
    " [ 1.99980262]]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** db **  </td>\n",
    "        <td> 0.499935230625 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** cost **  </td>\n",
    "        <td> 6.000064773192205</td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) 最佳化\n",
    "- 我們已經初始化了參數\n",
    "- 也計算了 cost 以及 gradient\n",
    "- 接下來就是透過 gradient descent的方式去更新參數\n",
    "\n",
    "**Exercise:** \n",
    "實作 optimization 函式. 目的是學習 $w$ and $b$ 來使得 cost function $J$ 得到之值最小. \n",
    "\n",
    "參數的更新規則為 $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, 其中 $\\alpha$ 是 learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.1124579 ]\n",
      " [0.23106775]]\n",
      "b = 1.5593049248448891\n",
      "dw = [[0.90158428]\n",
      " [1.76250842]]\n",
      "db = 0.4304620716786828\n",
      "[6.000064773192205]\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "    <tr>\n",
    "       <td> **w** </td>\n",
    "       <td>[[ 0.1124579 ]\n",
    " [ 0.23106775]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> **b** </td>\n",
    "       <td> 1.55930492484 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> **dw** </td>\n",
    "       <td> [[ 0.90158428]\n",
    " [ 1.76250842]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> **db** </td>\n",
    "       <td> 0.430462071679 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "透過optimize函式，我們可以得到學習好的 w 跟 b 參數，接下來請實作 `predict()` 函式，來使用學習好的w跟b進行預測。\n",
    "\n",
    "1. 計算 $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. 將activation函式(sigmoid即為一種activation函式)得到之結果轉換成label 0 或 1，若>0.5就是1，反之則為0，並將其儲存於向量 `Y_prediction` 之中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    ### END CODE HERE ###\n",
    "    print(A)\n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[0][i] > 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0][i] = 0\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99987661 0.99999386]]\n",
      "predictions = [[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "    <tr>\n",
    "         <td>\n",
    "             **predictions**\n",
    "         </td>\n",
    "          <td>\n",
    "            [[ 1.  1.]]\n",
    "         </td>  \n",
    "   </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color='blue'>\n",
    "目前已經實作了下列的函式:\n",
    "- Initialize (w,b)\n",
    "- Optimize the loss iteratively to learn parameters (w,b):\n",
    "- Use the learned (w,b) to predict the labels for a given set of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - 將上述函式組成一個Model ##\n",
    "\n",
    "**Exercise:** \n",
    "實作 `model` 函式，並使用下列的符號:\n",
    "    - 對test set的預測結果儲存於 Y_prediction\n",
    "    - 對train set的預測結果儲存於 Y_prediction_train\n",
    "    - outputs of optimize() 的回傳值儲存於 w, costs, grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = np.zeros((dim, 1)), 0.0\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = sigmoid()\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = \n",
    "    b = \n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = \n",
    "    Y_prediction_train = \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:40%\"> \n",
    "    \n",
    "    <tr>\n",
    "        <td> **Train Accuracy**  </td> \n",
    "        <td> 99.04306220095694 % </td>\n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td>**Test Accuracy** </td> \n",
    "        <td> 70.0 % </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example of a picture that was wrongly classified.\n",
    "index = 1\n",
    "plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\n",
    "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(d[\"Y_prediction_test\"][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也將 cost function 以及 gradient畫出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Further analysis (optional/ungraded exercise) ##\n",
    "\n",
    "Congratulations on building your first image classification model. Let's analyze it further, and examine possible choices for the learning rate $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of learning rate ####\n",
    "\n",
    "**Reminder**:\n",
    "In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may \"overshoot\" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.\n",
    "\n",
    "Let's compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the `learning_rates` variable to contain, and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Different learning rates give different costs and thus different predictions results.\n",
    "- If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). \n",
    "- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.\n",
    "- In deep learning, we usually recommend that you: \n",
    "    - Choose the learning rate that better minimizes the cost function.\n",
    "    - If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later videos.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7 - Test with your own image (optional/ungraded exercise) ##\n",
    "\n",
    "Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:\n",
    "    1. Click on \"File\" in the upper bar of this notebook, then click \"Open\" to go on your Coursera Hub.\n",
    "    2. Add your image to this Jupyter Notebook's directory, in the \"images\" folder\n",
    "    3. Change your image's name in the following code\n",
    "    4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## START CODE HERE ## (PUT YOUR IMAGE NAME) \n",
    "my_image = \"cat_in_iran.jpg\"   # change this to the name of your image file \n",
    "## END CODE HERE ##\n",
    "\n",
    "# We preprocess the image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\n",
    "my_predicted_image = predict(d[\"w\"], d[\"b\"], my_image)\n",
    "\n",
    "plt.imshow(image)\n",
    "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**What to remember from this assignment:**\n",
    "1. Preprocessing the dataset is important.\n",
    "2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().\n",
    "3. Tuning the learning rate (which is an example of a \"hyperparameter\") can make a big difference to the algorithm. You will see more examples of this later in this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you'd like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:\n",
    "    - Play with the learning rate and the number of iterations\n",
    "    - Try different initialization methods and compare the results\n",
    "    - Test other preprocessings (center the data, or divide each row by its standard deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
