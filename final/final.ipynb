{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba #裝繁體版本 https://github.com/APCLab/jieba-tw\n",
    "import jieba.posseg as pseg\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For RNN \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Concatenate, Embedding\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, History \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\asus\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['通姦在刑法上應該除罪化' '應該取消機車強制二段式左轉(待轉)' '支持博弈特區在台灣合法化' '中華航空空服員罷工是合理的'\n",
      " '性交易應該合法化' 'ECFA早收清單可（有）達到其預期成效' '應該減免證所稅' '贊成中油在觀塘興建第三天然氣接收站'\n",
      " '支持中國學生納入健保' '支持臺灣中小學（含高職、專科）服儀規定（含髮、襪、鞋）給予學生自主' '不支持使用加密貨幣' '不支持學雜費調漲'\n",
      " '同意政府舉債發展前瞻建設計畫' '支持電競列入體育競技' '反對台鐵東移徵收案' '支持陳前總統保外就醫'\n",
      " '年金改革應取消或應調降軍公教月退之優存利率十八趴' '同意動物實驗' '油價應該凍漲或緩漲' '反對旺旺中時併購中嘉']\n",
      "(20,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.847 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通姦 / 在 / 刑法 / 上應 / 該 / 除罪 / 化\n",
      "應該 / 取消 / 機車 / 強制 / 二段式 / 左轉 / ( / 待轉 / )\n",
      "支持 / 博弈 / 特區 / 在 / 台灣 / 合法化\n",
      "中華 / 航空 / 空服員 / 罷工 / 是 / 合理 / 的\n",
      "性交易 / 應該 / 合法化\n",
      "ECFA / 早 / 收清 / 單可 / （ / 有 / ） / 達到 / 其 / 預期 / 成效\n",
      "應該 / 減免 / 證所稅\n",
      "贊成 / 中油 / 在 / 觀塘 / 興建 / 第三 / 天然 / 氣 / 接收站\n",
      "支持 / 中國學 / 生納入 / 健保\n",
      "支持 / 臺 / 灣 / 中小 / 學 / （ / 含高職 / 、 / 專科 / ） / 服儀 / 規定 / （ / 含 / 髮 / 、 / 襪 / 、 / 鞋 / ） / 給予 / 學生 / 自主\n",
      "不 / 支持 / 使用 / 加密 / 貨幣\n",
      "不 / 支持 / 學雜費 / 調漲\n",
      "同意 / 政府 / 舉債 / 發展 / 前瞻 / 建設 / 計畫\n",
      "支持 / 電競 / 列入 / 體育 / 競技\n",
      "反對 / 台 / 鐵東移 / 徵收 / 案\n",
      "支持 / 陳 / 前 / 總統 / 保外 / 就醫\n",
      "年 / 金 / 改革 / 應 / 取消 / 或應 / 調降 / 軍 / 公教 / 月 / 退 / 之 / 優存 / 利率 / 十八 / 趴\n",
      "同意 / 動物 / 實驗\n",
      "油價 / 應該 / 凍漲 / 或 / 緩漲\n",
      "反對 / 旺旺 / 中時 / 併 / 購中 / 嘉\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n通姦 / 在 / 刑法 / 上 / 應該 / 除罪化\\n應該 / 取消 / 機車 / 強制 / 二段式 / 左轉 / ( / 待轉 / )\\n支持 / 博弈 / 特區 / 在 / 台灣 / 合法化\\n中華 / 航空 / 空服員 / 罷工 / 是 / 合理 / 的\\n性交易 / 應該 / 合法化\\nECFA / 早 / 收清單 / 可 / （ / 有 / ） / 達到 / 其 / 預期 / 成效\\n應該 / 減免 / 證所稅\\n贊成 / 中油 / 在 / 觀塘 / 興建 / 第三天 / 然氣 / 接收站\\n支持 / 中國 / 學生 / 納入 / 健保\\n支持 / 臺灣 / 中小學 / （ / 含 / 高職 / 、 / 專科 / ） / 服儀 / 規定 / （ / 含髮 / 、 / 襪 / 、 / 鞋 / ） / 給予 / 學生 / 自主\\n不 / 支持 / 使用 / 加密 / 貨幣\\n不 / 支持 / 學雜費 / 調漲\\n同意 / 政府 / 舉債 / 發展 / 前瞻 / 建設 / 計畫\\n支持 / 電競 / 列入 / 體育 / 競技\\n反對 / 台鐵 / 東移 / 徵收案\\n支持 / 陳前總統 / 保外 / 就醫\\n年金 / 改革 / 應 / 取消 / 或 / 應 / 調降 / 軍公教 / 月 / 退之 / 優存 / 利率 / 十八 / 趴\\n同意 / 動物 / 實驗\\n油價 / 應該 / 凍漲 / 或 / 緩漲\\n反對 / 旺旺中 / 時 / 併購 / 中嘉\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"QS_1.csv\") # 使用 pandas 讀取 CSV\n",
    "query = data.to_numpy()\n",
    "query = query[:, 1]\n",
    "print(query)\n",
    "print(query.shape)\n",
    "\n",
    "for element in query:\n",
    "    seg_list = jieba.cut(element)\n",
    "    print(\" / \".join(seg_list))\n",
    "    \n",
    "\"\"\"\n",
    "通姦 / 在 / 刑法 / 上 / 應該 / 除罪化\n",
    "應該 / 取消 / 機車 / 強制 / 二段式 / 左轉 / ( / 待轉 / )\n",
    "支持 / 博弈 / 特區 / 在 / 台灣 / 合法化\n",
    "中華 / 航空 / 空服員 / 罷工 / 是 / 合理 / 的\n",
    "性交易 / 應該 / 合法化\n",
    "ECFA / 早 / 收清單 / 可 / （ / 有 / ） / 達到 / 其 / 預期 / 成效\n",
    "應該 / 減免 / 證所稅\n",
    "贊成 / 中油 / 在 / 觀塘 / 興建 / 第三天 / 然氣 / 接收站\n",
    "支持 / 中國 / 學生 / 納入 / 健保\n",
    "支持 / 臺灣 / 中小學 / （ / 含 / 高職 / 、 / 專科 / ） / 服儀 / 規定 / （ / 含髮 / 、 / 襪 / 、 / 鞋 / ） / 給予 / 學生 / 自主\n",
    "不 / 支持 / 使用 / 加密 / 貨幣\n",
    "不 / 支持 / 學雜費 / 調漲\n",
    "同意 / 政府 / 舉債 / 發展 / 前瞻 / 建設 / 計畫\n",
    "支持 / 電競 / 列入 / 體育 / 競技\n",
    "反對 / 台鐵 / 東移 / 徵收案\n",
    "支持 / 陳前總統 / 保外 / 就醫\n",
    "年金 / 改革 / 應 / 取消 / 或 / 應 / 調降 / 軍公教 / 月 / 退之 / 優存 / 利率 / 十八 / 趴\n",
    "同意 / 動物 / 實驗\n",
    "油價 / 應該 / 凍漲 / 或 / 緩漲\n",
    "反對 / 旺旺中 / 時 / 併購 / 中嘉\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98250\n",
      "﻿news_000001\n",
      "通姦在刑法上應該除罪化\n",
      "姦\n",
      "True\n",
      "news_000001\n",
      "新北市第二選區議員候選人陳明義，爭取連任失利，成了落選頭，他認為選前清潔工事件影片遭人變造、剪接，影響選務人員公平性，將部分屬於他的選票認定為無效票或其他候選人的票，上月向新北地院聲請全面驗票，並對吊車尾當選的陳文治提出當選無效之訴。新北地院表示，日前確實收到陳明義的相關聲請，最快在今日就會分案，並擇期開庭，傳喚陳明義進一步說明，再認定是否有驗票必要。新北市第二選區議員候選人陳明義，去年九合一大新北市第二選區議員候選人陳明義，爭取連任失利，成了落選頭，他認為選前清潔工事件影片遭人變造、剪接，影響選務人員公平性，將部分屬於他的選票認定為無效票或其他候選人的票，上月向新北地院聲請全面驗票，並對吊車尾當選的陳文治提出當選無效之訴。新北地院表示，日前確實收到陳明義的相關聲請，最快在今日就會分案，並擇期開庭，傳喚陳明義進一步說明，再認定是否有驗票必要。新北市第二選區議員候選人陳明義，去年九合一大選前，因競選旗幟與布幕遭到清潔隊員拆除，他得知後怒嗆「讓你們掃個夠」，遭人錄下並上傳網路，引發社會輿論撻伐，更有上百名清潔工包圍陳明義競選總部。據了解，陳明義認為，該影片遭人剪接、變造，隨媒體強力放送，不僅重創他的形象，更影響選務人員的公平性，造成不少投給他的有效選票，遭選務人員認定為無效票，或是他人選票。陳明義指出，選務人員的不公，已構成《公職人員選舉罷免法》中的「當選票數不實，足認有影響選舉結果之虞」，向新北地院聲請對第二選區包括新莊、五股、林口與泰山共387個投開票所全面驗票。陳明義表示，上月委託2位律師研究，針對陳文治、張晉婷其中1人提出當選無效告訴，然因張已被檢方提出當選無效，才轉而對陳提出告訴；對於驗票與否，陳明義說，目前仍無法確認，全數細節仍待8日自上海返台後與律師討論。對此，市議員陳文治則說，「真金不怕火煉」，若要重新驗票就每個票匭逐一驗票，的確，票數贏的少，僅有15票之差，但也不會影響陳明義落選事實，何況，提出當選無效告訴、驗票與否，也是他的權利。\n",
      "\n",
      "新北市 / 第二 / 選區 / 議員候 / 選人 / 陳 / 明義 / ， / 爭取 / 連任 / 失利 / ， / 成 / 了 / 落 / 選頭 / ， / 他 / 認為 / 選 / 前清 / 潔工 / 事件 / 影片 / 遭人 / 變造 / 、 / 剪接 / ， / 影響 / 選務人員 / 公平性 / ， / 將 / 部分 / 屬 / 於 / 他 / 的 / 選票 / 認定 / 為 / 無效票 / 或 / 其他 / 候選人 / 的 / 票 / ， / 上 / 月 / 向 / 新北 / 地院 / 聲 / 請 / 全面 / 驗票 / ， / 並對 / 吊車 / 尾 / 當選 / 的 / 陳 / 文治 / 提出 / 當選無效 / 之訴 / 。 / 新北 / 地院 / 表示 / ， / 日前 / 確實 / 收到 / 陳 / 明義的 / 相關聲 / 請 / ， / 最快 / 在 / 今日 / 就會 / 分案 / ， / 並擇期 / 開庭 / ， / 傳喚 / 陳 / 明義進 / 一步 / 說明 / ， / 再 / 認定 / 是否 / 有驗票 / 必要 / 。 / 新北市 / 第二 / 選區 / 議員候 / 選人 / 陳 / 明義 / ， / 去年 / 九 / 合一 / 大新北市 / 第二 / 選區 / 議員候 / 選人 / 陳 / 明義 / ， / 爭取 / 連任 / 失利 / ， / 成 / 了 / 落 / 選頭 / ， / 他 / 認為 / 選 / 前清 / 潔工 / 事件 / 影片 / 遭人 / 變造 / 、 / 剪接 / ， / 影響 / 選務人員 / 公平性 / ， / 將 / 部分 / 屬 / 於 / 他 / 的 / 選票 / 認定 / 為 / 無效票 / 或 / 其他 / 候選人 / 的 / 票 / ， / 上 / 月 / 向 / 新北 / 地院 / 聲 / 請 / 全面 / 驗票 / ， / 並對 / 吊車 / 尾 / 當選 / 的 / 陳 / 文治 / 提出 / 當選無效 / 之訴 / 。 / 新北 / 地院 / 表示 / ， / 日前 / 確實 / 收到 / 陳 / 明義的 / 相關聲 / 請 / ， / 最快 / 在 / 今日 / 就會 / 分案 / ， / 並擇期 / 開庭 / ， / 傳喚 / 陳 / 明義進 / 一步 / 說明 / ， / 再 / 認定 / 是否 / 有驗票 / 必要 / 。 / 新北市 / 第二 / 選區 / 議員候 / 選人 / 陳 / 明義 / ， / 去年 / 九 / 合一 / 大選前 / ， / 因 / 競選旗幟 / 與 / 布幕 / 遭到 / 清潔 / 隊員 / 拆除 / ， / 他 / 得知 / 後 / 怒 / 嗆 / 「 / 讓 / 你 / 們 / 掃個夠 / 」 / ， / 遭人錄 / 下 / 並上 / 傳 / 網路 / ， / 引發 / 社會 / 輿論 / 撻 / 伐 / ， / 更 / 有 / 上 / 百名 / 清潔工 / 包圍 / 陳 / 明義競選 / 總部 / 。 / 據 / 了解 / ， / 陳 / 明義認 / 為 / ， / 該 / 影片 / 遭 / 人 / 剪接 / 、 / 變造 / ， / 隨媒體 / 強力 / 放送 / ， / 不僅 / 重創 / 他 / 的 / 形象 / ， / 更 / 影響 / 選務人員 / 的 / 公平性 / ， / 造成 / 不少 / 投給 / 他 / 的 / 有效 / 選票 / ， / 遭選務 / 人員 / 認定 / 為 / 無效票 / ， / 或是 / 他人 / 選票 / 。 / 陳 / 明義 / 指出 / ， / 選務人員 / 的 / 不公 / ， / 已構 / 成 / 《 / 公職 / 人員 / 選舉 / 罷免 / 法 / 》 / 中 / 的 / 「 / 當選票 / 數不實 / ， / 足 / 認有 / 影響 / 選舉 / 結果 / 之 / 虞 / 」 / ， / 向 / 新北 / 地院 / 聲 / 請 / 對 / 第二 / 選區 / 包括 / 新莊 / 、 / 五股 / 、 / 林口 / 與 / 泰山 / 共 / 387 / 個 / 投開票 / 所 / 全面 / 驗票 / 。 / 陳 / 明義 / 表示 / ， / 上月委 / 託 / 2 / 位律師 / 研究 / ， / 針對 / 陳 / 文治 / 、 / 張晉婷 / 其中 / 1 / 人 / 提出 / 當選無效 / 告訴 / ， / 然因 / 張 / 已 / 被 / 檢方 / 提出 / 當選無效 / ， / 才 / 轉而 / 對 / 陳 / 提出 / 告訴 / ； / 對 / 於 / 驗票 / 與 / 否 / ， / 陳 / 明義說 / ， / 目前 / 仍無法 / 確認 / ， / 全數 / 細節 / 仍 / 待 / 8 / 日自 / 上海 / 返台 / 後 / 與 / 律師 / 討論 / 。 / 對此 / ， / 市議員 / 陳 / 文治 / 則說 / ， / 「 / 真金不怕 / 火煉 / 」 / ， / 若 / 要 / 重新 / 驗票 / 就 / 每個 / 票 / 匭 / 逐一 / 驗票 / ， / 的確 / ， / 票數贏 / 的 / 少 / ， / 僅有 / 15 / 票之差 / ， / 但 / 也 / 不會 / 影響 / 陳 / 明義落 / 選事實 / ， / 何況 / ， / 提出 / 當選無效 / 告訴 / 、 / 驗票 / 與 / 否 / ， / 也 / 是 / 他 / 的 / 權利 / 。 / \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../../NC_1_ALL.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    AllNews = f.readlines()\n",
    "\n",
    "print(len(AllNews))\n",
    "print(AllNews[0][:12])\n",
    "#print(type(AllNews[0][:12]))\n",
    "print(query[0])\n",
    "print(query[0][1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(AllNews[0][1:12] == \"news_000001\") #some cant seen word in begin of string\n",
    "AllNews[0] = AllNews[0][1:]\n",
    "print(AllNews[0][:11])\n",
    "print(AllNews[0][14:])\n",
    "seg_list = jieba.cut(AllNews[0][14:])\n",
    "print(\" / \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalWordBag = {}\n",
    "counter = Counter()\n",
    "\n",
    "#加入停用字中\n",
    "stops = []\n",
    "stops.append('\\n') \n",
    "stops.append('\\n\\n')\n",
    "stops.append('2011')\n",
    "stops.append('-')\n",
    "stops.append('〔')\n",
    "stops.append('〕')\n",
    "stops.append('，')\n",
    "stops.append('「')\n",
    "stops.append('」')\n",
    "stops.append('。')\n",
    "stops.append('，')\n",
    "stops.append('；')\n",
    "stops.append('？')\n",
    "stops.append('：')\n",
    "stops.append('『')\n",
    "stops.append('』')\n",
    "stops.append('(')\n",
    "stops.append(')')\n",
    "stops.append('、')\n",
    "stops.append('！')\n",
    "stops.append('\\x08')\n",
    "stops.append('（')\n",
    "stops.append('）')\n",
    "stops.append('\\t')\n",
    "stops.append('/')\n",
    "stops.append(':')\n",
    "stops.append('【')\n",
    "stops.append('】')\n",
    "stops.append('╱')\n",
    "stops.append('')\n",
    "stops.append('的')\n",
    "stops.append('著')\n",
    "\n",
    "newsCut = {}\n",
    "\n",
    "for news in AllNews:\n",
    "    terms = [t for t in jieba.cut(news[14:], cut_all=True) if t not in stops]\n",
    "    counter.update(terms)\n",
    "    newsCut[news[:11]] = terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58678574\n",
      "('北市', 50660)\n",
      "94832\n"
     ]
    }
   ],
   "source": [
    "print(sum(counter.values()))\n",
    "#result = sorted(counter.items(), key=lambda x:x[1], reverse=True)\n",
    "result = list(counter.items())\n",
    "print(result[1])\n",
    "result = [item for item in result if item[1] > 1]\n",
    "print(len(result))\n",
    "#print(newsCut[\"news_000001\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "wordToIndex = {u[0]:i+1 for i, u in enumerate(result)} # ('北市', 50660) 的 (北市=>index)\n",
    "#del counter\n",
    "print(wordToIndex[\"北市\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Query   News_Index  Relevance\n",
      "0  支持陳前總統保外就醫  news_064209          2\n",
      "1  支持陳前總統保外就醫  news_012873          1\n",
      "2  支持陳前總統保外就醫  news_056765          3\n",
      "3  支持陳前總統保外就醫  news_055305          1\n",
      "4  支持陳前總統保外就醫  news_097573          3\n",
      "news_064209\n",
      "news_014437 Not in txt\n",
      "news_042122 Not in txt\n",
      "news_059972 Not in txt\n",
      "news_013409 Not in txt\n",
      "news_002482 Not in txt\n",
      "news_003104 Not in txt\n",
      "news_033917 Not in txt\n",
      "news_071107 Not in txt\n",
      "news_013721 Not in txt\n",
      "news_060673 Not in txt\n",
      "news_087071 Not in txt\n",
      "news_019490 Not in txt\n",
      "news_060908 Not in txt\n",
      "news_039945 Not in txt\n",
      "news_052382 Not in txt\n",
      "news_043689 Not in txt\n",
      "news_025079 Not in txt\n",
      "news_087627 Not in txt\n",
      "news_008873 Not in txt\n",
      "news_084450 Not in txt\n",
      "news_024997 Not in txt\n",
      "news_039932 Not in txt\n",
      "news_018139 Not in txt\n",
      "news_019353 Not in txt\n",
      "news_023689 Not in txt\n",
      "news_002871 Not in txt\n",
      "news_074771 Not in txt\n",
      "news_093716 Not in txt\n",
      "news_019620 Not in txt\n",
      "news_047619 Not in txt\n",
      "news_091442 Not in txt\n",
      "news_009393 Not in txt\n",
      "news_011881 Not in txt\n",
      "news_088144 Not in txt\n",
      "news_093139 Not in txt\n",
      "news_046284 Not in txt\n",
      "news_042942 Not in txt\n",
      "news_042959 Not in txt\n",
      "news_045942 Not in txt\n",
      "news_025305 Not in txt\n",
      "news_095517 Not in txt\n",
      "news_054560 Not in txt\n",
      "news_047791 Not in txt\n",
      "news_080722 Not in txt\n",
      "news_019759 Not in txt\n",
      "news_060908 Not in txt\n",
      "news_045513 Not in txt\n",
      "news_093143 Not in txt\n",
      "news_050926 Not in txt\n",
      "news_040047 Not in txt\n",
      "news_012858 Not in txt\n",
      "news_021827 Not in txt\n",
      "news_011711 Not in txt\n",
      "news_031213 Not in txt\n",
      "news_039838 Not in txt\n",
      "news_049612 Not in txt\n",
      "news_037035 Not in txt\n",
      "news_023832 Not in txt\n",
      "news_042887 Not in txt\n",
      "news_067891 Not in txt\n",
      "news_094273 Not in txt\n",
      "news_009141 Not in txt\n",
      "news_025353 Not in txt\n",
      "news_042859 Not in txt\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"TD.csv\") # 使用 pandas 讀取 CSV\n",
    "print(data.head())\n",
    "TD = data.to_numpy()\n",
    "trainQueryX = []\n",
    "trainNewsX = []\n",
    "dataY = []\n",
    "\n",
    "print(TD[0][1])\n",
    "for each in TD:\n",
    "    if each[1] in newsCut:\n",
    "        seg_list = jieba.cut(each[0])\n",
    "        _query = []\n",
    "        \n",
    "        _news = []\n",
    "        for _word in seg_list:\n",
    "            if _word in wordToIndex:\n",
    "                _query.append( wordToIndex[ _word ] )\n",
    "            else:\n",
    "                _query.append(0)\n",
    "        for i in range(len(newsCut[each[1]])):\n",
    "            if newsCut[each[1]][i] in wordToIndex:\n",
    "                _news.append( wordToIndex[ newsCut[each[1]][i] ] )\n",
    "            else:\n",
    "                _news.append(0)\n",
    "        trainQueryX.append(_query)\n",
    "        trainNewsX.append(_news)\n",
    "        dataY.append(each[2])\n",
    "    else:\n",
    "        print( each[1] + \" Not in txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1251, 10, 98, 0, 4605, 0]\n",
      "[3606, 6478, 6479, 1203, 656, 168, 304, 14059, 4550, 841, 221, 521, 10, 11061, 794, 795, 171, 36, 139, 5210, 487, 2520, 1364, 3582, 1353, 504, 745, 1195, 2612, 114, 98, 138, 455, 10, 2026, 4596, 5203, 700, 652, 653, 4605, 77, 654, 14059, 4550, 841, 2811, 75, 3606, 2241, 166, 658, 168, 304, 10, 11061, 266, 1200, 284, 7, 10799, 10666, 280, 1665, 1287, 6, 521, 5100, 7837, 18, 238, 266, 6, 7, 183, 7456, 1892, 168, 103, 975, 264, 2814, 355, 934, 1132, 547, 280, 101, 8102, 58746, 7197, 1110, 5157, 355, 280, 2071, 628, 10, 11061, 62, 264, 746, 121, 3435, 28565, 1359, 3782, 1424, 89, 652, 653, 75, 3346, 1750, 126, 1347, 3803, 672, 1022, 6772, 7185, 2428, 1204, 2028, 628, 111, 1424, 103, 1200, 1101, 108, 78, 1306, 1894, 652, 653, 24, 940, 114, 3611, 940, 11662, 2523, 1366, 14059, 4550, 841, 111, 3606, 6478, 6479, 1203, 656, 168, 304, 14059, 4550, 841, 221, 521, 10, 11061, 794, 795, 171, 36, 139, 5210, 487, 2520, 1364, 3582, 1353, 504, 745, 1195, 2612, 114, 98, 138, 455, 10, 2026, 4596, 5203, 700, 652, 653, 4605, 77, 654, 14059, 4550, 841, 2811, 75, 3606, 2241, 166, 658, 168, 304, 10, 11061, 266, 1200, 284, 7, 10799, 10666, 280, 1665, 1287, 6, 521, 5100, 7837, 18, 238, 266, 6, 7, 183, 7456, 1892, 168, 103, 975, 264, 2814, 355, 934, 1132, 547, 280, 101, 8102, 58746, 7197, 1110, 5157, 355, 280, 2071, 628, 10, 11061, 62, 264, 746, 121, 3435, 28565, 1359, 3782, 1424, 89, 652, 653, 75, 3346, 1750, 126, 1347, 3803, 672, 1022, 6772, 7185, 2428, 1204, 2028, 628, 111, 1424, 103, 1200, 1101, 108, 78, 1306, 1894, 652, 653, 24, 940, 114, 3611, 940, 11662, 2523, 1366, 14059, 4550, 841, 111, 10, 11061, 505, 973, 404, 794, 795, 171, 36, 139, 5210, 487, 2520, 1364, 3582, 1353, 504, 745, 1195, 2612, 114, 10, 2026, 4596, 5203, 700, 652, 653, 4605, 77, 654, 1424, 679, 89, 10, 2026, 4596, 700, 652, 653, 111, 1424, 78, 716, 3739, 973, 571, 189, 7789, 10, 11061, 57, 58, 1183, 5210, 487, 2520, 77, 10, 2026, 4596, 4605, 77, 654, 80, 2539, 742, 6, 3014, 92, 480, 4766, 1424, 162, 654, 3316, 247, 248, 164, 23, 44, 3878, 4596, 940, 4605, 77, 654, 5210, 487, 2520, 859, 99, 1195, 10049, 114, 1183, 142, 7910, 10, 2026, 4596, 1133, 646, 213, 75, 1704, 172, 45, 547, 2366, 2621, 243, 103, 9694, 2360, 3606, 6, 78, 5791, 238, 75, 3606, 12772, 1327, 168, 304, 14059, 4550, 841, 5100, 7837, 18, 103, 372, 953, 6, 7, 10799, 10666, 280, 1665, 57, 1110, 50, 975, 264, 8102, 58746, 7197, 280, 101, 3641, 18276]\n",
      "4153\n",
      "16\n",
      "[0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#print(newsCut[\"news_097573\"])\n",
    "print(trainQueryX[4])\n",
    "print(trainNewsX[4])\n",
    "\n",
    "cMax = 0\n",
    "for news in trainNewsX:\n",
    "    if cMax < len(news):\n",
    "        cMax = len(news)\n",
    "print(cMax)\n",
    "\n",
    "cMax = 0\n",
    "for news in trainQueryX:\n",
    "    if cMax < len(news):\n",
    "        cMax = len(news)\n",
    "print(cMax)\n",
    "\n",
    "# one-hot encoding\n",
    "trainY = to_categorical(dataY, num_classes=4) # 0 1 2 3\n",
    "#y_test = to_categorical(y_test, num_classes=4)\n",
    "print(trainY[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4678, 32)\n",
      "(4678, 5120)\n"
     ]
    }
   ],
   "source": [
    "# 要 zero padding 成過定長度\n",
    "#https://datascience.stackexchange.com/questions/26366/training-an-rnn-with-examples-of-different-lengths-in-keras\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/\n",
    "\n",
    "# 16\n",
    "trainQueryX = pad_sequences(trainQueryX, padding='post', maxlen=32)\n",
    "print(trainQueryX.shape)\n",
    "\n",
    "#4153\n",
    "trainNewsX = pad_sequences(trainNewsX, padding='post', maxlen=5120)\n",
    "print(trainNewsX.shape)\n",
    "#trainQueryX = np.reshape(trainQueryX, (trainQueryX.shape[0], trainQueryX.shape[1], 1))\n",
    "#trainNewsX = np.reshape(trainNewsX, (trainNewsX.shape[0], trainNewsX.shape[1], 1))\n",
    "\n",
    "# 總共有 4678 筆，每筆不同長度，每次一筆(每個字視為一個 feature) -> 每筆回傳一個 0~3 的判斷\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_Query, X_test_Query, y_train, y_test = train_test_split(trainQueryX, trainY, test_size=0.3, random_state=41)\n",
    "X_train_News, X_test_News, y_train, y_test = train_test_split(trainNewsX, trainY, test_size=0.3, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________\n",
      "Layer (type)                 Output Shape        Param #    Connected to                  \n",
      "==========================================================================================\n",
      "input_3 (InputLayer)         (None, 32)          0                                        \n",
      "__________________________________________________________________________________________\n",
      "input_4 (InputLayer)         (None, 5120)        0                                        \n",
      "__________________________________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 32, 128)     12138624   input_3[0][0]                 \n",
      "__________________________________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 5120, 128)   12138624   input_4[0][0]                 \n",
      "__________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)          20608      embedding_3[0][0]             \n",
      "__________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)         394240     embedding_4[0][0]             \n",
      "__________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)  (None, 288)         0          lstm_3[0][0]                  \n",
      "                                                            lstm_4[0][0]                  \n",
      "__________________________________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)          9248       concatenate_2[0][0]           \n",
      "__________________________________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)           132        dense_3[0][0]                 \n",
      "==========================================================================================\n",
      "Total params: 24,701,476\n",
      "Trainable params: 24,701,476\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#input (query, documents) output 0~3\n",
    "#https://blog.csdn.net/churximi/article/details/61210129\n",
    "\n",
    "def rnn(vocab_size, embedding_dim, seq_len=1000):\n",
    "    #(4678, 不定長度)\n",
    "    inputA = Input(shape=(32, )) # 假設 1000 個詞(zero padding)\n",
    "    x = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputA)\n",
    "    inputB = Input(shape=(5120, ))\n",
    "    y = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputB)\n",
    "    #本來 LSTM 吃的 input: (N 個, 每個長度, 每個 feature 數量)\n",
    "    #這邊是把 (N, 32) -> embedding -> (N, 32, embedding_dim)\n",
    "    #有種 [0][0] = 1 -> [0][0] = (1, 1, 1, 1, ...., 1)\n",
    "    #不然本來要 [0][0] = 1 -> [0][0][0] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # the first branch operates on the first input\n",
    "    #(batch_size, timesteps, input_dim)\n",
    "    x = LSTM(32)(x)\n",
    "    \n",
    "    # the second branch opreates on the second input\n",
    "    y = LSTM(256)(y)\n",
    "    \n",
    "    # combine the output of the two branches\n",
    "    combined = Concatenate()([x, y])\n",
    "\n",
    "    # apply a FC layer and then a regression prediction on the\n",
    "    # combined outputs\n",
    "    z = Dense(32, activation=\"relu\")(combined)\n",
    "    z = Dense(4, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    # our model will accept the inputs of the two branches and\n",
    "    # then output a single value\n",
    "    model = Model(inputs=[inputA, inputB], outputs=z)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = rnn(len(result)+1, 128)\n",
    "print(model.summary(90))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "WARNING:tensorflow:From C:\\Users\\asus\\Miniconda3\\envs\\testJ\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3274 samples, validate on 1404 samples\n",
      "Epoch 1/10\n",
      "3274/3274 [==============================] - 1566s 478ms/step - loss: 1.2121 - acc: 0.4435 - val_loss: 1.1173 - val_acc: 0.5064\n",
      "Epoch 2/10\n",
      "3274/3274 [==============================] - 1678s 513ms/step - loss: 1.0671 - acc: 0.5342 - val_loss: 1.2425 - val_acc: 0.2472\n",
      "Epoch 3/10\n",
      "3274/3274 [==============================] - 1567s 479ms/step - loss: 1.1698 - acc: 0.4551 - val_loss: 1.1431 - val_acc: 0.5128\n",
      "Epoch 4/10\n",
      "3274/3274 [==============================] - 1509s 461ms/step - loss: 0.9116 - acc: 0.5977 - val_loss: 1.2369 - val_acc: 0.4793\n",
      "Epoch 5/10\n",
      "3274/3274 [==============================] - 1505s 460ms/step - loss: 0.7001 - acc: 0.7193 - val_loss: 1.4533 - val_acc: 0.4466\n",
      "Epoch 6/10\n",
      "3274/3274 [==============================] - 1506s 460ms/step - loss: 0.5251 - acc: 0.8033 - val_loss: 1.5683 - val_acc: 0.4523\n",
      "Epoch 7/10\n",
      "3274/3274 [==============================] - 1504s 459ms/step - loss: 0.3723 - acc: 0.8662 - val_loss: 1.9233 - val_acc: 0.4330\n",
      "Epoch 8/10\n",
      "3274/3274 [==============================] - 1505s 460ms/step - loss: 0.2582 - acc: 0.9114 - val_loss: 2.1180 - val_acc: 0.4679\n",
      "Epoch 9/10\n",
      "3274/3274 [==============================] - 1504s 459ms/step - loss: 0.1932 - acc: 0.9307 - val_loss: 2.3395 - val_acc: 0.4480\n",
      "Epoch 10/10\n",
      "3274/3274 [==============================] - 1504s 459ms/step - loss: 0.1283 - acc: 0.9597 - val_loss: 2.5843 - val_acc: 0.4501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca87acb9b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "\n",
    "#https://keras.io/zh/callbacks/\n",
    "#keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "modelCheck = ModelCheckpoint('LSTM{epoch:08d}.h5', save_weights_only=True, save_best_only=True)\n",
    "#keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "earlyStop = EarlyStopping(monitor='val_loss', patience=50)\n",
    "history = History()\n",
    "\n",
    "model.fit([X_train_Query, X_train_News], y_train, \n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=([X_test_Query, X_test_News], y_test),\n",
    "          callbacks=[modelCheck, earlyStop, history])\n",
    "    #,validation_data=([testAttrX, testImagesX], testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/10\n",
      "4678/4678 [==============================] - 2043s 437ms/step - loss: 0.6294 - acc: 0.8053\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Miniconda3\\envs\\testJ\\lib\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4678/4678 [==============================] - 2028s 434ms/step - loss: 0.3891 - acc: 0.8775\n",
      "Epoch 3/10\n",
      "4678/4678 [==============================] - 1906s 407ms/step - loss: 0.2631 - acc: 0.9196\n",
      "Epoch 4/10\n",
      "4678/4678 [==============================] - 1905s 407ms/step - loss: 0.1911 - acc: 0.9389\n",
      "Epoch 5/10\n",
      "4678/4678 [==============================] - 1906s 407ms/step - loss: 0.1468 - acc: 0.9547\n",
      "Epoch 6/10\n",
      "4678/4678 [==============================] - 1908s 408ms/step - loss: 0.1211 - acc: 0.9617\n",
      "Epoch 7/10\n",
      "4678/4678 [==============================] - 1909s 408ms/step - loss: 0.1009 - acc: 0.9669\n",
      "Epoch 8/10\n",
      "4678/4678 [==============================] - 1913s 409ms/step - loss: 0.0748 - acc: 0.9741\n",
      "Epoch 9/10\n",
      "4678/4678 [==============================] - 1906s 407ms/step - loss: 0.0584 - acc: 0.9820\n",
      "Epoch 10/10\n",
      "4678/4678 [==============================] - 1964s 420ms/step - loss: 0.0488 - acc: 0.9835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca87acbb00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "\n",
    "#https://keras.io/zh/callbacks/\n",
    "#keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "modelCheck = ModelCheckpoint('LSTM_NO_VAL{epoch:08d}.h5', save_weights_only=True, save_best_only=True)\n",
    "#keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "earlyStop = EarlyStopping(monitor='val_loss', patience=50)\n",
    "history = History()\n",
    "\n",
    "model.fit([trainQueryX, trainNewsX], trainY, \n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[modelCheck, history])\n",
    "    #,validation_data=([testAttrX, testImagesX], testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_064209\n",
      "6442\n",
      "6890\n",
      "5275\n",
      "6453\n",
      "5761\n",
      "6837\n",
      "11578\n",
      "5263\n",
      "5997\n",
      "7057\n",
      "7275\n",
      "7733\n",
      "8231\n",
      "6507\n",
      "6663\n",
      "7514\n",
      "5365\n",
      "6492\n",
      "5137\n",
      "9075\n",
      "8169\n",
      "5279\n",
      "10428\n",
      "7585\n",
      "5208\n",
      "5145\n",
      "7580\n",
      "6969\n",
      "6130\n",
      "8122\n",
      "5122\n",
      "5386\n",
      "5786\n",
      "6366\n",
      "5766\n",
      "6867\n",
      "5255\n",
      "11576\n",
      "5199\n",
      "9563\n",
      "8715\n",
      "5236\n",
      "7574\n"
     ]
    }
   ],
   "source": [
    "#fully process\n",
    "#turn all news to vector\n",
    "\n",
    "testNewsX = []\n",
    "\n",
    "print(TD[0][1])\n",
    "for news in AllNews:\n",
    "    if news[:11] in newsCut:\n",
    "        _news = []\n",
    "        for i in range(len(newsCut[news[:11]])):\n",
    "            if newsCut[news[:11]][i] in wordToIndex:\n",
    "                _news.append( wordToIndex[ newsCut[news[:11]][i] ] )\n",
    "            else:\n",
    "                _news.append(0)\n",
    "        if(len(_news) > 5120):\n",
    "            print(news[:11])\n",
    "            print(len(_news))\n",
    "        else:\n",
    "            testNewsX.append(_news)\n",
    "    else:\n",
    "        print( each[1] + \" Not in txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6442\n",
      "6890\n",
      "5275\n",
      "6453\n",
      "5761\n",
      "6837\n",
      "11578\n",
      "5263\n",
      "5997\n",
      "7057\n",
      "7275\n",
      "7733\n",
      "8231\n",
      "6507\n",
      "6663\n",
      "7514\n",
      "5365\n",
      "6492\n",
      "5137\n",
      "9075\n",
      "8169\n",
      "5279\n",
      "10428\n",
      "7585\n",
      "5208\n",
      "5145\n",
      "7580\n",
      "6969\n",
      "6130\n",
      "8122\n",
      "5122\n",
      "5386\n",
      "5786\n",
      "6366\n",
      "5766\n",
      "6867\n",
      "5255\n",
      "11576\n",
      "5199\n",
      "9563\n",
      "8715\n",
      "5236\n",
      "7574\n",
      "11578\n"
     ]
    }
   ],
   "source": [
    "testMax = 0\n",
    "for element in testNewsX:\n",
    "    if(len(element) > testMax):\n",
    "        testMax = len(element)\n",
    "    if(len(element) > 5120):\n",
    "        print(len(element))\n",
    "print(testMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn query to vector\n",
    "allQueryCut = []   \n",
    "for element in query:\n",
    "    seg_list = jieba.cut(element)\n",
    "    _query = []\n",
    "    for _word in seg_list:\n",
    "        if _word in wordToIndex:\n",
    "            _query.append( wordToIndex[ _word ] )\n",
    "        else:\n",
    "            _query.append(0)\n",
    "    allQueryCut.append(_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[0, 75, 9726, 0, 142, 10038, 649]\n",
      "98207\n"
     ]
    }
   ],
   "source": [
    "allQueryCut = np.array(allQueryCut)\n",
    "print(len(allQueryCut))\n",
    "print(allQueryCut[0])\n",
    "print(len(testNewsX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0    75  9726 ...   142 10038   649]\n",
      " [    0    75  9726 ...   142 10038   649]\n",
      " [    0    75  9726 ...   142 10038   649]\n",
      " ...\n",
      " [    0    75  9726 ...   142 10038   649]\n",
      " [    0    75  9726 ...   142 10038   649]\n",
      " [    0    75  9726 ...   142 10038   649]]\n"
     ]
    }
   ],
   "source": [
    "#把每一筆抓出來建立整個長度的複製\n",
    "query_1_X = np.array([allQueryCut[0]])\n",
    "query_1_X = np.repeat(query_1_X, [98207], axis=0)\n",
    "print(query_1_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98207/98207 [==============================] - 13147s 134ms/step\n"
     ]
    }
   ],
   "source": [
    "query_1_X = pad_sequences(query_1_X, padding='post', maxlen=32)\n",
    "testNewsX = pad_sequences(testNewsX, padding='post', maxlen=5120)\n",
    "\n",
    "q1result = model.predict([query_1_X, testNewsX], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4053793e-04 8.8502055e-01 7.3882937e-04 2.3797154e-04]\n",
      " [2.3129582e-04 1.9631457e-01 3.2583773e-03 1.3151765e-04]\n",
      " [2.8210163e-02 3.6771542e-01 6.0207069e-02 2.4238527e-03]\n",
      " [1.7073426e-01 3.8785994e-02 2.4257302e-02 3.5274029e-04]\n",
      " [6.3630939e-04 9.8658413e-02 3.4915179e-02 1.1253953e-03]\n",
      " [6.8247318e-06 7.5489283e-05 2.3558736e-04 8.5958862e-01]\n",
      " [9.3373060e-03 4.9413294e-02 3.1966120e-02 8.7706149e-03]\n",
      " [1.0311604e-05 2.4232355e-01 1.5037954e-03 3.3599138e-04]\n",
      " [2.9221177e-04 4.5322984e-01 6.4130127e-03 1.5336275e-03]\n",
      " [6.6279471e-03 4.1341269e-01 3.2921731e-03 1.8486381e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(q1result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_000493\n",
      "6442\n",
      "news_001830\n",
      "6890\n",
      "news_005859\n",
      "5275\n",
      "news_006624\n",
      "6453\n",
      "news_009760\n",
      "5761\n",
      "news_011974\n",
      "6837\n",
      "news_013296\n",
      "11578\n",
      "news_013986\n",
      "5263\n",
      "news_015160\n",
      "5997\n",
      "news_018618\n",
      "7057\n",
      "news_019024\n",
      "7275\n",
      "news_021565\n",
      "7733\n",
      "news_022512\n",
      "8231\n",
      "news_022617\n",
      "6507\n",
      "news_027638\n",
      "6663\n",
      "news_031533\n",
      "7514\n",
      "news_031565\n",
      "5365\n",
      "news_036580\n",
      "6492\n",
      "news_038126\n",
      "5137\n",
      "news_041235\n",
      "9075\n",
      "news_044698\n",
      "8169\n",
      "news_048946\n",
      "5279\n",
      "news_050600\n",
      "10428\n",
      "news_051081\n",
      "7585\n",
      "news_055783\n",
      "5208\n",
      "news_064355\n",
      "5145\n",
      "news_065503\n",
      "7580\n",
      "news_068718\n",
      "6969\n",
      "news_071636\n",
      "6130\n",
      "news_074446\n",
      "8122\n",
      "news_075524\n",
      "5122\n",
      "news_079978\n",
      "5386\n",
      "news_080177\n",
      "5786\n",
      "news_082890\n",
      "6366\n",
      "news_085389\n",
      "5766\n",
      "news_086699\n",
      "6867\n",
      "news_089666\n",
      "5255\n",
      "news_089781\n",
      "11576\n",
      "news_096070\n",
      "5199\n",
      "news_096395\n",
      "9563\n",
      "news_098508\n",
      "8715\n",
      "news_099607\n",
      "5236\n",
      "news_099764\n",
      "7574\n"
     ]
    }
   ],
   "source": [
    "indexToNews = []\n",
    "for news in AllNews:\n",
    "    if news[:11] in newsCut:\n",
    "        _news = []\n",
    "        for i in range(len(newsCut[news[:11]])):\n",
    "            if newsCut[news[:11]][i] in wordToIndex:\n",
    "                _news.append( wordToIndex[ newsCut[news[:11]][i] ] )\n",
    "            else:\n",
    "                _news.append(0)\n",
    "        if(len(_news) > 5120):\n",
    "            print(news[:11])\n",
    "            print(len(_news))\n",
    "        else:\n",
    "            indexToNews.append(news[:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n",
      "[['news_000588', 0.97795266], ['news_000686', 0.9706557], ['news_000801', 0.9817699], ['news_000892', 0.9466374], ['news_000972', 0.9801428]]\n"
     ]
    }
   ],
   "source": [
    "q1_1_count = 0\n",
    "Q1result = []\n",
    "for i in range(len(q1result)):\n",
    "    if q1result[i][3] > 0.915:\n",
    "        q1_1_count += 1\n",
    "        Q1result.append([indexToNews[i], q1result[i][3]])\n",
    "print(q1_1_count)\n",
    "print(Q1result[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('query1.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['新聞', '機率'])\n",
    "    for word, num in Q1result:\n",
    "        writer.writerow([word, num])\n",
    "\n",
    "\n",
    "with open('query1total2.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['新聞', '機率0', '機率1', '機率2', '機率3'])\n",
    "    for i in range(len(q1result)):\n",
    "        writer.writerow([indexToNews[i], q1result[i][0], q1result[i][1], q1result[i][2], q1result[i][3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98207/98207 [==============================] - 12465s 127ms/step\n"
     ]
    }
   ],
   "source": [
    "model2 = rnn(len(result)+1, 128)\n",
    "model2.load_weights(\"LSTM00000001.h5\")\n",
    "q1resultv2 = model2.predict([query_1_X, testNewsX], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553\n",
      "[['news_000058', 0.25337654], ['news_000402', 0.2396459], ['news_000485', 0.23715794], ['news_000960', 0.22012448], ['news_001409', 0.26034182]]\n"
     ]
    }
   ],
   "source": [
    "q1_2_count = 0\n",
    "Q1resultv2 = []\n",
    "for i in range(len(q1resultv2)):\n",
    "    if q1resultv2[i][3] > 0.22:\n",
    "        q1_2_count += 1\n",
    "        Q1resultv2.append([indexToNews[i], q1resultv2[i][3]])\n",
    "print(q1_2_count)\n",
    "print(Q1resultv2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 發現長度要 12288 才夠\n",
    "\n",
    "def rnn(vocab_size, embedding_dim, seq_len=1000):\n",
    "    #(4678, 不定長度)\n",
    "    inputA = Input(shape=(32, )) # 假設 1000 個詞(zero padding)\n",
    "    x = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputA)\n",
    "    inputB = Input(shape=(12288, ))\n",
    "    y = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputB)\n",
    "    #本來 LSTM 吃的 input: (N 個, 每個長度, 每個 feature 數量)\n",
    "    #這邊是把 (N, 32) -> embedding -> (N, 32, embedding_dim)\n",
    "    #有種 [0][0] = 1 -> [0][0] = (1, 1, 1, 1, ...., 1)\n",
    "    #不然本來要 [0][0] = 1 -> [0][0][0] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # the first branch operates on the first input\n",
    "    #(batch_size, timesteps, input_dim)\n",
    "    x = LSTM(32)(x)\n",
    "    \n",
    "    # the second branch opreates on the second input\n",
    "    y = LSTM(256)(y)\n",
    "    \n",
    "    # combine the output of the two branches\n",
    "    combined = Concatenate()([x, y])\n",
    "\n",
    "    # apply a FC layer and then a regression prediction on the\n",
    "    # combined outputs\n",
    "    z = Dense(32, activation=\"relu\")(combined)\n",
    "    z = Dense(4, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    # our model will accept the inputs of the two branches and\n",
    "    # then output a single value\n",
    "    model = Model(inputs=[inputA, inputB], outputs=z)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = rnn(len(result)+1, 128)\n",
    "print(model.summary(90))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 16\n",
    "trainQueryX = pad_sequences(trainQueryX, padding='post', maxlen=32)\n",
    "print(trainQueryX.shape)\n",
    "\n",
    "#12288\n",
    "trainNewsX = pad_sequences(trainNewsX, padding='post', maxlen=12288)\n",
    "print(trainNewsX.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_Query, X_test_Query, y_train, y_test = train_test_split(trainQueryX, trainY, test_size=0.3, random_state=41)\n",
    "X_train_News, X_test_News, y_train, y_test = train_test_split(trainNewsX, trainY, test_size=0.3, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "modelCheck = ModelCheckpoint('LSTM12288_{epoch:02d}.h5', save_weights_only=True, save_best_only=True)\n",
    "history = History()\n",
    "\n",
    "model.fit([X_train_Query, X_train_News], y_train, \n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          validation_data=([X_test_Query, X_test_News], y_test),\n",
    "          callbacks=[modelCheck, history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0\n",
      " 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "Q1vectors = np.zeros((230,11790), dtype=np.int)\n",
    "totalWordBagArray = list(totalWordBag.keys())\n",
    "for i in range(len(newsList)):\n",
    "    newsDict = newsList[i]\n",
    "    for j in range(len(totalWordBagArray) ):\n",
    "        if totalWordBagArray[j] in newsDict:\n",
    "            Q1vectors[i][j] = 1\n",
    "\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(Q1vectors[229])\n",
    "    \n",
    "if True:\n",
    "    np.savetxt(\"Q1vectors.csv\", Q1vectors, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(\"./\" + name + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_obj(name):\n",
    "    with open(\"./\" + name + \".pkl\", \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "save_obj(totalWordBag, \"Q1wordBag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a2494a0a14ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'詞彙'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'出現次數'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('query1.txt', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['詞彙', '出現次數'])\n",
    "    for word, num in result[:100]:\n",
    "        writer.writerow([word, num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\asus\\Miniconda3\\envs\\testJ\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "(32, 10)\n",
      "(32, 10, 64)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length=10))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "# [32*10]\n",
    "print(input_array.shape)\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "assert output_array.shape == (32, 10, 64)\n",
    "print(output_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\asus\\Miniconda3\\envs\\testJ\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-046de31c64ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#x = LSTM(8, return_sequences=True)(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\testJ\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m                                          \u001b[1;34m'You can build it manually via: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m--> 431\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\testJ\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mchild_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(None, 5)))\n",
    "model.add(LSTM(8, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(2, activation='sigmoid')))\n",
    "\"\"\"\n",
    "\n",
    "inputA = Input(shape=(None, 5)) # 假設 1000 個詞(zero padding)\n",
    "x = LSTM(32)(inputA)\n",
    "#x = LSTM(8, return_sequences=True)(x)\n",
    "x = TimeDistributed(Dense(2, activation='sigmoid'))(x)\n",
    "model = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "print(model.summary(100))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam')\n",
    "\n",
    "def train_generator():\n",
    "    while True:\n",
    "        sequence_length = np.random.randint(10, 100) # 長度隨機介於 10~100\n",
    "        print(sequence_length)\n",
    "        x_train = np.random.random((1000, sequence_length, 5))# [1000][固定長度][5]\n",
    "        # y_train will depend on past 5 timesteps of x\n",
    "        y_train = x_train[:, :, 0]\n",
    "        for i in range(1, 5):\n",
    "            y_train[:, i:] += x_train[:, :-i, i]\n",
    "        y_train = to_categorical(y_train > 2.5)\n",
    "        yield x_train, y_train\n",
    "\n",
    "#model.fit_generator(train_generator(), steps_per_epoch=30, epochs=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
